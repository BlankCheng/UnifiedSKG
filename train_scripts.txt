################################## WTQ #####################################
# finetune wtq with BART-base
python -m torch.distributed.launch \
    --nproc_per_node 4 \
    --master_port 1234 train.py \
    --seed 2 \
    --cfg Salesforce/BART_base_finetune_wikitq.cfg \
    --run_name BART_base_finetune_wikitq \
    --logging_strategy steps \
    --logging_first_step true \
    --logging_steps 4 \
    --evaluation_strategy steps \
    --eval_steps 500 \
    --metric_for_best_model avr \
    --greater_is_better true \
    --save_strategy steps \
    --save_steps 500 \
    --save_total_limit 1 \
    --load_best_model_at_end \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 50 \
    --adafactor true \
    --learning_rate 5e-5 \
    --do_train --do_eval --do_predict \
    --predict_with_generate \
    --output_dir output/BART_base_finetune_wikitq \
    --overwrite_output_dir \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 16 \
    --generation_num_beams 4 \
    --generation_max_length 128 \
    --input_max_length 512 \
    --ddp_find_unused_parameters true


# finetune wtq with T5-base
python -m torch.distributed.launch \
    --nproc_per_node 4 \
    --master_port 1234 train.py \
    --seed 2 \
    --cfg Salesforce/T5_base_finetune_wikitq.cfg \
    --run_name T5_base_finetune_wikitq \
    --logging_strategy steps \
    --logging_first_step true \
    --logging_steps 4 \
    --evaluation_strategy steps \
    --eval_steps 500 \
    --metric_for_best_model avr \
    --greater_is_better true \
    --save_strategy steps \
    --save_steps 500 \
    --save_total_limit 1 \
    --load_best_model_at_end \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 50 \
    --adafactor true \
    --learning_rate 5e-5 \
    --do_train --do_eval --do_predict \
    --predict_with_generate \
    --output_dir output/T5_base_finetune_wikitq \
    --overwrite_output_dir \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 16 \
    --generation_num_beams 4 \
    --generation_max_length 128 \
    --input_max_length 512 \
    --ddp_find_unused_parameters true


# prefixtune wtq with BART-base
python -m torch.distributed.launch \
    --nproc_per_node 4 \
    --master_port 1234 train.py \
    --seed 2 \
    --cfg Salesforce/BART_base_prefixtune_wikitq.cfg \
    --run_name BART_base_prefixtune_wikitq \
    --logging_strategy steps \
    --logging_first_step true \
    --logging_steps 4 \
    --evaluation_strategy steps \
    --eval_steps 500 \
    --metric_for_best_model avr \
    --greater_is_better true \
    --save_strategy steps \
    --save_steps 500 \
    --save_total_limit 1 \
    --load_best_model_at_end \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 50 \
    --adafactor true \
    --learning_rate 5e-5 \
    --do_train --do_eval --do_predict \
    --predict_with_generate \
    --output_dir output/BART_base_prefixtune_wikitq \
    --overwrite_output_dir \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 16 \
    --generation_num_beams 4 \
    --generation_max_length 128 \
    --input_max_length 512 \
    --ddp_find_unused_parameters true


# prefixtune wtq with T5-base
python -m torch.distributed.launch \
    --nproc_per_node 4 \
    --master_port 1234 train.py \
    --seed 2 \
    --cfg Salesforce/T5_base_prefix_wikitq.cfg \
    --run_name T5_base_prefix_wikitq \
    --logging_strategy steps \
    --logging_first_step true \
    --logging_steps 4 \
    --evaluation_strategy steps \
    --eval_steps 500 \
    --metric_for_best_model avr \
    --greater_is_better true \
    --save_strategy steps \
    --save_steps 500 \
    --save_total_limit 1 \
    --load_best_model_at_end \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 50 \
    --adafactor true \
    --learning_rate 5e-5 \
    --do_train --do_eval --do_predict \
    --predict_with_generate \
    --output_dir output/T5_base_prefix_wikitq \
    --overwrite_output_dir \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 16 \
    --generation_num_beams 4 \
    --generation_max_length 128 \
    --input_max_length 512 \
    --ddp_find_unused_parameters true



################################## HiTab #####################################
# finetune hitab with T5-base
python -m torch.distributed.launch \
    --nproc_per_node 4 \
    --master_port 1234 train.py \
    --seed 2 \
    --cfg Salesforce/T5_base_finetune_hitab.cfg \
    --run_name T5_base_finetune_hitab \
    --logging_strategy steps \
    --logging_first_step true \
    --logging_steps 4 \
    --evaluation_strategy steps \
    --eval_steps 500 \
    --metric_for_best_model avr \
    --greater_is_better true \
    --save_strategy steps \
    --save_steps 500 \
    --save_total_limit 1 \
    --load_best_model_at_end \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 50 \
    --adafactor true \
    --learning_rate 5e-5 \
    --do_train --do_eval --do_predict \
    --predict_with_generate \
    --output_dir output/T5_base_finetune_hitab \
    --overwrite_output_dir \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 16 \
    --generation_num_beams 4 \
    --generation_max_length 128 \
    --input_max_length 512 \
    --ddp_find_unused_parameters true




################################## Pretrain #####################################
# pretrain finetune wikitables with T5-base
python -m torch.distributed.launch \
    --nproc_per_node 4 \
    --master_port 1234 train.py \
    --seed 2 \
    --cfg Salesforce/pretrain/T5_base_finetune_wikitables_mlm.cfg \
    --run_name T5_base_finetune_wikitables_mlm \
    --logging_strategy steps \
    --logging_first_step true \
    --logging_steps 4 \
    --evaluation_strategy steps \
    --eval_steps 500 \
    --metric_for_best_model avr \
    --greater_is_better true \
    --save_strategy steps \
    --save_steps 500 \
    --save_total_limit 1 \
    --load_best_model_at_end \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 1 \
    --adafactor true \
    --learning_rate 5e-5 \
    --do_train --do_eval --do_predict \
    --predict_with_generate \
    --output_dir output/T5_base_finetune_wikitables_mlm \
    --overwrite_output_dir \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 16 \
    --generation_num_beams 4 \
    --generation_max_length 256 \
    --input_max_length 256 \
    --ddp_find_unused_parameters true


# pretrain finetune wikitables with BART-base
python -m torch.distributed.launch \
    --nproc_per_node 4 \
    --master_port 1234 train.py \
    --seed 2 \
    --cfg Salesforce/pretrain/BART_base_finetune_wikitables_mlm.cfg \
    --run_name BART_base_finetune_wikitables_mlm \
    --logging_strategy steps \
    --logging_first_step true \
    --logging_steps 4 \
    --evaluation_strategy steps \
    --eval_steps 500 \
    --metric_for_best_model avr \
    --greater_is_better true \
    --save_strategy steps \
    --save_steps 500 \
    --save_total_limit 1 \
    --load_best_model_at_end \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 1 \
    --adafactor true \
    --learning_rate 5e-5 \
    --do_train --do_eval --do_predict \
    --predict_with_generate \
    --output_dir output/BART_base_finetune_wikitables_mlm \
    --overwrite_output_dir \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 16 \
    --generation_num_beams 4 \
    --generation_max_length 512 \
    --input_max_length 512 \
    --ddp_find_unused_parameters true


# pretrain plugintune wikitables with BART-base
python -m torch.distributed.launch \
    --nproc_per_node 1 \
    --master_port 1234 train.py \
    --seed 2 \
    --cfg Salesforce/pretrain/BART_base_plugintune_wikitables_mlm.cfg \
    --run_name BART_base_plugintune_wikitables_mlm \
    --logging_strategy steps \
    --logging_first_step true \
    --logging_steps 4 \
    --evaluation_strategy steps \
    --eval_steps 2 \
    --metric_for_best_model avr \
    --greater_is_better true \
    --save_strategy steps \
    --save_steps 500 \
    --save_total_limit 1 \
    --load_best_model_at_end \
    --gradient_accumulation_steps 8 \
    --num_train_epochs 1 \
    --adafactor true \
    --learning_rate 5e-5 \
    --do_train --do_eval --do_predict \
    --predict_with_generate \
    --output_dir output/BART_base_plugintune_wikitables_mlm \
    --overwrite_output_dir \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 16 \
    --generation_num_beams 4 \
    --generation_max_length 512 \
    --input_max_length 512 \
    --ddp_find_unused_parameters true
